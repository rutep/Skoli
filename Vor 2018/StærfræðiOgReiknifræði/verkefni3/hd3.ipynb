{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REI201G Heimaverkefni 3\n",
    "\n",
    "Þriðji heimadæmaskammtur er úr köflum 4 og 5 í kennslubók og á að skila dæmunum föstudaginn 2. mars. Lausnum á að skila á Gradescope í formi Jupyter vinnubókar á PDF sniði (Velja File/Download as HTML, opna skrá í vafra og prenta í PDF skrá). \n",
    "\n",
    "Til að skrá sig inn á námskeiðið í Gradescope þarf að slá inn kóða 9KYPRX.\n",
    "* Athugið að merkja allar lausnir vandlega með nafni og hi.is notendanafni.\n",
    "* Merkið við hvar hvert dæmi er að finna á síðunum.\n",
    "* Ef engu er skilað fyrir eitthvað dæmi þarf að merkja það sem autt.\n",
    "\n",
    "Í dæmatíma í næstu viku verður farið yfir nokkur tímadæmi en restin af tímanum verður svo notuð til að vinna í heimadæmum.\n",
    "\n",
    "Eftirfarandi reglur gilda um heimadæmaskil:\n",
    "\n",
    "Þeir nemendur sem vinna að lausn heimadæma með öðrum þurfa alltaf að skrifa upp og skila inn sinni eigin lausn. Þeir þurfa ennfremur að tilgreina með hverjum var unnið að lausn verkefnisins. Það er óheimilt að fá lausnir hjá öðrum, afrita lausnir eða láta aðra fá lausnina sína. Ef kennari verður var við afritaðar lausnir mun hann lækka einkunn fyrir viðkomandi verkefni. Hikið ekki við að leita til kennara ef þið eruð í vafa um hvað telst eðlileg samvinna og hvað ekki.\n",
    "\n",
    "*Athugið*: Til að hljóta próftökurétt þarf að skila 3 heimaverkefnum af fyrstu 5 og hljóta að lágmarki 5.0 í meðaleinkunn fyrir 3 bestu verkefnin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heimadæmi\n",
    "\n",
    "Dæmi 1 og 2 eru úr kennslubók en dæmi 3 og 4 eru úr dæmahefti sem fylgir bókinni (http://stanford.edu/class/ee103/103exercises.pdf).\n",
    "\n",
    "1\\. Dæmi 5.2 í bók.\n",
    "\n",
    "\n",
    "2\\. Dæmi 5.8 í bók. Rökstyðjið svör ykkar stuttlega með vísun í eiginleika Gram-Schmidt reikniritsins.\n",
    "\n",
    "\n",
    "3\\. *Meðmælakerfi sem byggir á $k$-means.* Streymiþjónusta fyrir tónlist hefur $N$ notendur sem hlusta á lög úr safni með $n$ lögum yfir ákveðið tímabil (t.d. einn mánuð). Tónlistarvali notanda $i$ er lýst með $n$-vigri (lagalista) $p_i$ sem er skilgreindur þannig,\n",
    "$$\n",
    "(p_i)_j = \\left\\{ \\begin{array}{ll}1 & \\textrm{notandi}~i~\\textrm{hefur spilað lag}~j\\\\ \n",
    "                                   0 & \\textrm{notandi}~i~\\textrm{hefur ekki spilað lag}~j \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "fyrir $j=1,\\ldots,n$. (Athugið að $p_i$ er $n$-vigur en $(p_i)_j$ er tala.) Gerið ráð fyrir að ef notandi hlustar á tiltekið lag þá finnst honum það skemmtilegt.\n",
    "\n",
    "Verkefni þitt er að hanna reiknirit sem mælir með 10 lögum við sérhvern notanda, lög sem hann hefur ekki hlustað á en gæti fundist skemmtileg. (Það má gera ráð fyrir að það séu a.m.k. 10 lög sem sérhver notandi hefur ekki hlustað á.)\n",
    "\n",
    "Til að gera þetta þá keyrir þú $k$-means reikniritið á lagalistavigrum $p_1,\\ldots,p_N$ (t.d. með $k=100$). Þá fást miðpunktar $z_1,\\ldots z_k$ sem eru $n$-vigrar.\n",
    "\n",
    "Hver eru næstu skref? Útskýrðu með eigin **orðum**, frekar en að setja fram formúlu eða forritskóða, hvernig þú mælir með lögum við sérhvern notanda.\n",
    "\n",
    "4\\. *Topic discovery via $k$-means*. In this problem you will use $k$-means to cluster 300 Wikipedia articles selected from 5 broad groups of topics. The Numpy file `wikipedia_corpus.npz` contains three arrays which you access as follows\n",
    "\n",
    "```python\n",
    "data=numpy.load('wikipedia_corpus.npz')\n",
    "dictionary = data[\"dictionary\"]\n",
    "article_titles = data[\"article_titles\"]\n",
    "article_histograms = data[\"article_histograms\"]```\n",
    "\n",
    "The data consists of word histograms, 300 1000-vectors in the variable `article_histograms`. It also provides an array of article titles in `article_titles` and an array of the 1000 words used to create the histograms in `dictionary`.\n",
    "\n",
    "The ﬁle `kmeans.py` provides a Python implementation of the $k$-means algorithm in the function `kmeans`. The `kmeans` function accepts a list of vectors to cluster (in the form of a matrix) along with the number of clusters, $k$, and returns three things: the centroids as an array of vectors (a matrix), an array containing the index of each vector’s closest centroid, and an array of the value of $J$ after each iteration of $k$-means. Each time the function `kmeans` is invoked it initializes the centroids by randomly assigning the data points to $k$ groups and taking the $k$ representatives as the means of the groups. (This means that if you run `kmeans` twice, with the same data, you might get diﬀerent results.)\n",
    "\n",
    "For example, here is an example of running $k$-means with $k = 8$ and ﬁnding the 30th article’s centroid (recall indices start from zero).\n",
    "\n",
    "```python\n",
    "from kmeans import kmeans\n",
    "centroids, labels, j_hist = kmeans(article_histograms, 8)\n",
    "centroids[labels[29]]```\n",
    "\n",
    "The array `labels` contains the index of each vector’s closest centroid (labels start from zero), so if the 30th entry in `labels` is 7, then the 30th vector’s closest centroid is the (7+1)-th entry in `centroids`.\n",
    "\n",
    "There are many ways to explore your results. For example, you could print the titles of all articles in a cluster.\n",
    "\n",
    "```python\n",
    "In [ ] article_titles[labels == 7]\n",
    "Out[ ] array(['Antenna (radio)', 'Attenuation', 'Broadcasting', ..., dtype=object])```\n",
    "\n",
    "Alternatively, you could ﬁnd a topic’s most common words by ordering `dictionary` by the size of its centroid’s entries. A larger entry for a word implies it was more common in articles from that topic.\n",
    "\n",
    "```python\n",
    "In [ ] dictionary[np.argsort(centroids[7])[-10:]] # the top ten words\n",
    "Out[ ] array(['wireless', 'communication', 'transmission', 'antenna', 'waves',\n",
    "       'optical', 'telegraph', 'signal', 'fiber', 'radio'], dtype=object)```\n",
    "\n",
    "a) For each of $k = 2,~k = 5$, and $k = 10$ run $k$-means twice, and plot $J$ (vertically) versus iteration (horizontally) for the two runs on the same plot. Create your plot by passing a vector containing the value of $J$ at each iteration to the `matplotlib.plot` function (see example in the `vika_01` notebook). Comment brieﬂy on your results. \n",
    "\n",
    "b) Choose a value of $k$ from part (a) and investigate your results by looking at the words and article titles associated with each centroid. Feel free to visit Wikipedia if an article’s content is unclear from its title. Give a short description of the topics your clustering discovered along with the 3 most common words from each topic. If the topics do not make sense pick another value of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_histograms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7a4aadc8d0d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcentroids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_histograms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'article_histograms' is not defined"
     ]
    }
   ],
   "source": [
    "from kmeans import kmeans\n",
    "centroids, labels, j_hist = kmeans(article_histograms, 8)\n",
    "centroids[labels[29]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "data=numpy.load('wikipedia_corpus.npz')\n",
    "dictionary = data[\"dictionary\"]\n",
    "article_titles = data[\"article_titles\"]\n",
    "article_histograms = data[\"article_histograms\"]\n",
    "from kmeans import kmeans\n",
    "centroids, labels, j_hist = kmeans(article_histograms, 8)\n",
    "centroids[labels[29]]\n",
    "x =   article_titles[labels == 7]\n",
    "# array(['Antenna (radio)', 'Attenuation', 'Broadcasting', ..., dtype=object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   The k-means function clusters a dataset using the \n",
    "   k-means algorithm, covered in the book in section 4.3.\n",
    "\n",
    "   Specifically, it implements the following:\n",
    "\n",
    "   Randomly assign data points to centroids\n",
    "   Intialize centroids as mean of initially assigned data points\n",
    "   Repeat until convergence:\n",
    "     Assign each vector to its nearest centroid (partition_data)\n",
    "     Update each centroid to be the average of its assigned vectors (update_centroids)\n",
    "   \n",
    "   Parameters:\n",
    "     data - The vectors to cluster, represented as an array of N n-vectors\n",
    "            (N column vectors of length n).\n",
    "     k    - The number of clusters\n",
    "\n",
    "   Returns:\n",
    "     centroids - The cluster centroids represented as an array of k column vectors of length n.\n",
    "     labels    - An array of N cluster assignments in the same order as data, so that\n",
    "                 the 1st data point's cluster assignment is labels[i]. The labels themselves\n",
    "                 index into the centroids list, i.e. centroids[labels[1]] returns the\n",
    "                 first data point's centroid.\n",
    "     losses    - An array containing the value of J at each iteration.\n",
    "\n",
    "usage:\n",
    "# A simple 2D dataset with 2 clearly separated clusters\n",
    "X = np.array([[1,1],[2,1],[1.5,3], [5,5],[6,5],[5.5,7]])\n",
    "k = 2\n",
    "centroids, labels, losses = kmeans(data, k)\n",
    "for i in range(k):\n",
    "    print(\"label=\", i)\n",
    "    print(data[labels==i])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:,0], X[:,1], c=labels)\n",
    "plt.show()\n",
    "\n",
    "    This is a Python port of kmeans.jl from http://stanford.edu/class/ee103/julia_files/kmeans.jl\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def kmeans(data, k):\n",
    "    epsilon = 1e-6\n",
    "    losses = []\n",
    "    N,n = data.shape\n",
    "    centroids, labels = initialize_centroids(data, k)\n",
    "    while True:\n",
    "        labels = partition_data(data, centroids)\n",
    "        centroids = update_centroids(data, labels, k, centroids)\n",
    "        losses.append(loss(data,centroids,labels))\n",
    "        if len(losses) >= 2 and abs(losses[-1] - losses[-2]) <= epsilon:\n",
    "            break\n",
    "    return centroids, labels, np.array(losses)\n",
    "\n",
    "def partition_data(data, centroids):\n",
    "    # Assigns each data point to the closest centroid\n",
    "    N = data.shape[0]\n",
    "    labels = np.zeros(N, dtype=np.int)\n",
    "    for i in range(N):\n",
    "       # calculate the euclidean distance from the ith data point to each centroid\n",
    "       centroid_distances = pairwise_distance(data[i,:], centroids)\n",
    "       # find the centroid closest to the ith data point\n",
    "       labels[i] = np.argmin(centroid_distances)\n",
    "    return labels\n",
    "\n",
    "def update_centroids(data, labels, k, old_centroids):\n",
    "    # Calculates each centroid as the mean of it's data points\n",
    "    centroids = np.zeros(shape=(k, data.shape[1]))\n",
    "    for i in range(k):\n",
    "       #get the data points assigned to the centroid\n",
    "       centroid_pts = data[labels == i] #[np.nonzero(labels == i), :]\n",
    "       # if the centroid has assigned data points, set it to be their mean\n",
    "       if centroid_pts.shape[0] > 0:\n",
    "           centroids[i,:]=np.average(centroid_pts, axis=0)\n",
    "       else:\n",
    "           # otherwise, leave it alone\n",
    "           centroids[i,:]=old_centroids[i,:]\n",
    "    return centroids\n",
    "\n",
    "def loss(data, centroids, labels):\n",
    "   # Calculates the loss (J) of the given clustering.\n",
    "   N = data.shape[0]\n",
    "   sos_dist = 0.0\n",
    "   for i in range(N):\n",
    "       #print(\"label=\",labels[i])\n",
    "       sos_dist += np.linalg.norm(data[i,:] - centroids[labels[i],:])**2\n",
    "   return sos_dist*(1/N)\n",
    "\n",
    "def initialize_centroids(data, k):\n",
    "    # Picks the starting centroids by randomly assigning data points to each centroid.\n",
    "    N,n = data.shape\n",
    "    labels = np.random.randint(k, size=N)\n",
    "    centroids = update_centroids(data, labels, k, np.zeros(shape=(k,n)))\n",
    "    return centroids, labels\n",
    "\n",
    "def pairwise_distance(vector, vectors):\n",
    "    # Calculates the Eudclidean distance between vector and each vectors in 'vectors', returning an array.\n",
    "    dist = np.zeros(vectors.shape[0])\n",
    "    for i in range(vectors.shape[0]):\n",
    "        dist[i] = np.linalg.norm(vector - vectors[i,:])\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'j_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f6db0ba5d83d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_hist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'j_hist' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tímadæmi\n",
    "\n",
    "Dæmi 4.2 í bók.\n",
    "\n",
    "Dæmi 5.1 í bók.\n",
    "\n",
    "Dæmi 5.6 í bók."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
