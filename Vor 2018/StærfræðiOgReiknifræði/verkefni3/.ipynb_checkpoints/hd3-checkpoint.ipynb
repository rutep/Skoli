{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REI201G Heimaverkefni 3\n",
    "\n",
    "Þriðji heimadæmaskammtur er úr köflum 4 og 5 í kennslubók og á að skila dæmunum föstudaginn 2. mars. Lausnum á að skila á Gradescope í formi Jupyter vinnubókar á PDF sniði (Velja File/Download as HTML, opna skrá í vafra og prenta í PDF skrá). \n",
    "\n",
    "Til að skrá sig inn á námskeiðið í Gradescope þarf að slá inn kóða 9KYPRX.\n",
    "* Athugið að merkja allar lausnir vandlega með nafni og hi.is notendanafni.\n",
    "* Merkið við hvar hvert dæmi er að finna á síðunum.\n",
    "* Ef engu er skilað fyrir eitthvað dæmi þarf að merkja það sem autt.\n",
    "\n",
    "Í dæmatíma í næstu viku verður farið yfir nokkur tímadæmi en restin af tímanum verður svo notuð til að vinna í heimadæmum.\n",
    "\n",
    "Eftirfarandi reglur gilda um heimadæmaskil:\n",
    "\n",
    "Þeir nemendur sem vinna að lausn heimadæma með öðrum þurfa alltaf að skrifa upp og skila inn sinni eigin lausn. Þeir þurfa ennfremur að tilgreina með hverjum var unnið að lausn verkefnisins. Það er óheimilt að fá lausnir hjá öðrum, afrita lausnir eða láta aðra fá lausnina sína. Ef kennari verður var við afritaðar lausnir mun hann lækka einkunn fyrir viðkomandi verkefni. Hikið ekki við að leita til kennara ef þið eruð í vafa um hvað telst eðlileg samvinna og hvað ekki.\n",
    "\n",
    "*Athugið*: Til að hljóta próftökurétt þarf að skila 3 heimaverkefnum af fyrstu 5 og hljóta að lágmarki 5.0 í meðaleinkunn fyrir 3 bestu verkefnin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heimadæmi\n",
    "\n",
    "Dæmi 1 og 2 eru úr kennslubók en dæmi 3 og 4 eru úr dæmahefti sem fylgir bókinni (http://stanford.edu/class/ee103/103exercises.pdf).\n",
    "\n",
    "1\\. Dæmi 5.2 í bók.\n",
    "\n",
    "\n",
    "2\\. Dæmi 5.8 í bók. Rökstyðjið svör ykkar stuttlega með vísun í eiginleika Gram-Schmidt reikniritsins.\n",
    "\n",
    "\n",
    "3\\. *Meðmælakerfi sem byggir á $k$-means.* Streymiþjónusta fyrir tónlist hefur $N$ notendur sem hlusta á lög úr safni með $n$ lögum yfir ákveðið tímabil (t.d. einn mánuð). Tónlistarvali notanda $i$ er lýst með $n$-vigri (lagalista) $p_i$ sem er skilgreindur þannig,\n",
    "$$\n",
    "(p_i)_j = \\left\\{ \\begin{array}{ll}1 & \\textrm{notandi}~i~\\textrm{hefur spilað lag}~j\\\\ \n",
    "                                   0 & \\textrm{notandi}~i~\\textrm{hefur ekki spilað lag}~j \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "fyrir $j=1,\\ldots,n$. (Athugið að $p_i$ er $n$-vigur en $(p_i)_j$ er tala.) Gerið ráð fyrir að ef notandi hlustar á tiltekið lag þá finnst honum það skemmtilegt.\n",
    "\n",
    "Verkefni þitt er að hanna reiknirit sem mælir með 10 lögum við sérhvern notanda, lög sem hann hefur ekki hlustað á en gæti fundist skemmtileg. (Það má gera ráð fyrir að það séu a.m.k. 10 lög sem sérhver notandi hefur ekki hlustað á.)\n",
    "\n",
    "Til að gera þetta þá keyrir þú $k$-means reikniritið á lagalistavigrum $p_1,\\ldots,p_N$ (t.d. með $k=100$). Þá fást miðpunktar $z_1,\\ldots z_k$ sem eru $n$-vigrar.\n",
    "\n",
    "Hver eru næstu skref? Útskýrðu með eigin **orðum**, frekar en að setja fram formúlu eða forritskóða, hvernig þú mælir með lögum við sérhvern notanda.\n",
    "\n",
    "4\\. *Topic discovery via $k$-means*. In this problem you will use $k$-means to cluster 300 Wikipedia articles selected from 5 broad groups of topics. The Numpy file `wikipedia_corpus.npz` contains three arrays which you access as follows\n",
    "\n",
    "```python\n",
    "data=numpy.load('wikipedia_corpus.npz')\n",
    "dictionary = data[\"dictionary\"]\n",
    "article_titles = data[\"article_titles\"]\n",
    "article_histograms = data[\"article_histograms\"]```\n",
    "\n",
    "The data consists of word histograms, 300 1000-vectors in the variable `article_histograms`. It also provides an array of article titles in `article_titles` and an array of the 1000 words used to create the histograms in `dictionary`.\n",
    "\n",
    "The ﬁle `kmeans.py` provides a Python implementation of the $k$-means algorithm in the function `kmeans`. The `kmeans` function accepts a list of vectors to cluster (in the form of a matrix) along with the number of clusters, $k$, and returns three things: the centroids as an array of vectors (a matrix), an array containing the index of each vector’s closest centroid, and an array of the value of $J$ after each iteration of $k$-means. Each time the function `kmeans` is invoked it initializes the centroids by randomly assigning the data points to $k$ groups and taking the $k$ representatives as the means of the groups. (This means that if you run `kmeans` twice, with the same data, you might get diﬀerent results.)\n",
    "\n",
    "For example, here is an example of running $k$-means with $k = 8$ and ﬁnding the 30th article’s centroid (recall indices start from zero).\n",
    "\n",
    "```python\n",
    "from kmeans import kmeans\n",
    "centroids, labels, j_hist = kmeans(article_histograms, 8)\n",
    "centroids[labels[29]]```\n",
    "\n",
    "The array `labels` contains the index of each vector’s closest centroid (labels start from zero), so if the 30th entry in `labels` is 7, then the 30th vector’s closest centroid is the (7+1)-th entry in `centroids`.\n",
    "\n",
    "There are many ways to explore your results. For example, you could print the titles of all articles in a cluster.\n",
    "\n",
    "```python\n",
    "In [ ]  article_titles[labels == 7]\n",
    "Out[ ] array(['Antenna (radio)', 'Attenuation', 'Broadcasting', ..., dtype=object])```\n",
    "\n",
    "Alternatively, you could ﬁnd a topic’s most common words by ordering `dictionary` by the size of its centroid’s entries. A larger entry for a word implies it was more common in articles from that topic.\n",
    "\n",
    "```python\n",
    "In [ ] dictionary[np.argsort(centroids[7])[-10:]] # the top ten words\n",
    "Out[ ] array(['wireless', 'communication', 'transmission', 'antenna', 'waves',\n",
    "       'optical', 'telegraph', 'signal', 'fiber', 'radio'], dtype=object)```\n",
    "\n",
    "a) For each of $k = 2,~k = 5$, and $k = 10$ run $k$-means twice, and plot $J$ (vertically) versus iteration (horizontally) for the two runs on the same plot. Create your plot by passing a vector containing the value of $J$ at each iteration to the `matplotlib.plot` function (see example in the `vika_01` notebook). Comment brieﬂy on your results. \n",
    "\n",
    "b) Choose a value of $k$ from part (a) and investigate your results by looking at the words and article titles associated with each centroid. Feel free to visit Wikipedia if an article’s content is unclear from its title. Give a short description of the topics your clustering discovered along with the 3 most common words from each topic. If the topics do not make sense pick another value of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tímadæmi\n",
    "\n",
    "Dæmi 4.2 í bók.\n",
    "\n",
    "Dæmi 5.1 í bók.\n",
    "\n",
    "Dæmi 5.6 í bók."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
